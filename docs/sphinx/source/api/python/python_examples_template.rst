:orphan: 

Python Examples
---------------

Constructing an Additive Tree Ensemble or `AddTree`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Veritas uses its own tree ensemble representation. You can manually build one to try Veritas out.

Here's an example of a manually constructed tree ensemble.
(To execute this code, see `tests/test_readme.py`.)

!code PART example_at!

This outputs the following. Note that the Boolean split on feature 2 is replaced with a less than split splitting on value 0.5 (``veritas.BOOL_SPLIT_VALUE``). You can use the pre-defined domains for `TRUE` and `FALSE`: ``veritas.TRUE_DOMAIN`` and ``veritas.FALSE_DOMAIN``.

!output PART example_at!


Model Conversion of Sklearn-model
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can also convert an existing ensemble using the ``get_addtree`` function for XGBoost, LightGBM and scikit-learn.

Here's an example of a model trained by a RandomForestClassifier that has been converted to Veritas' own tree ensemble representation.

!code PART get_addtree_example!

The output is an AddTree consisting of 3 trees, as was defined in the RandomForestClassifier.

!output PART get_addtree_example!


Model Conversion implementation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Converting representations of other learners or your own models should be easy and can be done by implementing the class ``AddTreeConverter``.
In the following example ``MyAddTreeConverter`` implements the ``get_addtree`` method from ``AddTreeConverter`` for a trivial tree representation. The trees consist of a boolean split in the root with only 2 leaves. After adding an instance of ``MyAddTreeConverter`` to the convertermanager, the same method ``get_addtree`` that was used in the previous example can be used for the new model representation aswell as the previously methoned ones.

!code PART AddTreeConverter!

This has the expected output:

!output PART AddTreeConverter!


Finding the Global Maximum of the Ensemble
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We can use Veritas to find the feature values for which the model's output is maximal as follows.

!code PART max_output!

!output PART max_output!

The solutions generated by ``Search`` are accessible using ``get_solution``. The solutions are sorted descendingly: the best solution is at index 0, the worst solution is at index ``s.num_solutions()-1``.

A best solution at index 0 is optimal when ``s.is_optimal()`` returns true. To know when the solution was generated, ``sol.time`` contains the number of seconds since the construction of the ``Search`` object.

The ``sol.box()`` method returns the value intervals of the features for which the output of the ensemble is unchanged. That is, for each possible assignment within the intervals, the trees always evaluate to the same leaf node (``s.get_solution_nodes``), and thus to the same output value. If a feature is missing from the box, it means that its value does not make a difference.


Constrained Minimization
^^^^^^^^^^^^^^^^^^^^^^^^

In this example, we constrain the first feature value to be between 3 and 5.
Because this is a very simple constraint, we can simply prune the search space before we start the search.

Although the constraint is simple, it is very useful. The exact same pruning strategy is used for l-infinity robustenss checking.

!code PART min_output_constrained!

!output PART min_output_constrained!

We minimize by maximizing the negated ensemble, i.e., the ensemble where all leaf values are negated.

The pruning simply removes all leaf nodes with boxes that do not overlap with ``prune_box`` from the search.


Contrasting Two Instances
^^^^^^^^^^^^^^^^^^^^^^^^^

In this example, we want to know what the maximum difference between the outputs of two instances can be when only the third feature is different, and first and second feature values are the same.

We achieve this by renaming the feature IDs in one of the trees using a feature map or ``FeatMap`` object.

!code PART featmap!

!output PART featmap!

There are two differences between tree 1 and tree 3:

- the leaf values are negated (``concat_negated``)
- internal node 6 uses feature ID 2 in tree 1 and feature ID 5 in tree 3

The other feature IDs are the same. This has the effect of allowing the first two trees (corresponding to the first instance) to take on different values for feature 3 than the last two trees (corresponding to the second instance).

The renaming of the feature IDs is fascilitated by the ``FeatMap`` object.

!code PART print_featmap!

!output PART print_featmap!

The above gives all IDs used by the two instances. ``FeatMap::share_all_features_between_instances`` can be used share all feature values between the two intances. By default, each ID is unique.
Use ``FeatMap::use_same_id_for`` to share the same ID for two features, either between two instances, or for the same instance.
Use ``FeatMap::transform`` to apply the changes to an ``AddTree``.

We can find the maximum difference between the outputs of the first and the second instance as follows:

!code PART two_instances!

!output PART two_instances!

The maximum output difference in this case is 10. The only possible variation is between leaf nodes 7 or 8 in the second tree.

Use ``Search::step_for(duration_in_seconds, num_steps)`` to let the search run for the given duration. Per ``num_steps`` steps, a snapshot is added to ``Search::snapshots``. This can be used to track the following stats:

- time (``time``)
- number of steps executed so far (``num_steps``)
- number of solutions so far (``num_solutions``)
- number of search states expanded so far (``num_states``)
- best epsilon value (``eps``)
- the best bounds so far (``bounds``), a tuple containing lower bound, A\* upper bound, and ARA\* upper bound


Checking Robustness
^^^^^^^^^^^^^^^^^^^

Before we check the robustness of a particular example, we'll first use Veritas to enumerate all possible output configurations of the additive tree ensemble. To do this, we simply run the search until ``Search::steps`` returns false, indicating that all search states have been visited.

!code PART robustness0!

!output PART robustness0!

The boxes above partition the input space. Remember that when a feature is not present in a box, it does not have an effect given the other feature values and can take on any value.

We will pick an example from box 6 with output -9:

!code PART robustness0_eval!

!output PART robustness0_eval!

We now try to find the distance to the closest adversarial example for which the output of the model is positive. We use ``VeritasRobustnessSearch`` for this. The arguments are:

- model to minimize or None
- model to maximize or None (use both for targeted attacks)
- the example
- the initial delta value used by the binary search

!code PART robustness1!

!output PART robustness1!

We can verify this result using the MILP approach (Kantchelian et al.'16):

!code PART robustness1_kan!

!output PART robustness1_kan LINES 1:2!

MILP indeed finds the same solution.


One-hot constraint
^^^^^^^^^^^^^^^^^^

We can tell Veritas that some of the features are the results of a one-hot encoded categorical feature using ``Search::add_onehot_constraint``. This ensures that exactly one of the features is true at all times.

For this constructed example with only two one-hot encoded features, the total number of solutions is four, but two of them are invalid:

!code PART onehot0!

!output PART onehot0!

When we inform Veritas that exactly one of the two features must be true:

!code PART onehot1!

!output PART onehot1!
