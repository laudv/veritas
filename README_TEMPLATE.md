[![PyPi Version](https://img.shields.io/pypi/v/dtai-veritas)](https://pypi.org/project/dtai-veritas/)

# Versatile Verification of Tree Ensembles with VERITAS

[View API documentation](https://alexandersch12.github.io/veritas/) | [Veritas in action blog post](https://dtai.cs.kuleuven.be/sports/blog/versatile-verification-of-soccer-analytics-models/)

## Installation

- Clone this repository: `git clone https://github.com/laudv/veritas.git`
- Change directory: `cd veritas`
- Initialize the [pybind11](https://pybind11.readthedocs.io) submodule `git submodule init` and `git submodule update`
- If you use environments: activate a (new) Python3 environment (e.g. using `venv`: `python3 -m venv venv_name && source venv_name/bin/activate`)
- run `pip install .` in the root directory of Veritas

Veritas should work on Linux (GCC), Mac (LLVM), and Windows (MSVC). If you encounter issues, feel free to contact me or open an issue.

To pull the latest updates from Github, simply `git pull` the changes and reinstall using `pip`: `pip install --force-reinstall .`.

## Constructing an Additive Tree Ensemble or `AddTree`

You can convert an existing ensemble using the `get_addtree` function for XGBoost, LightGBM and scikit-learn.

Here's an example of a model trained by XGBoost that has been converted to Veritas' own tree ensemble representation.

!code PART get_addtree_example!

The output is an AddTree consisting of 3 trees, as was defined in the XGBClassifier.

!output PART get_addtree_example!

## Queries

Starting from this AddTree, we can perform several queries.

!output PART example_at!

### Finding the Global Maximum of the Ensemble

We can use Veritas to find the feature values for which the model's output is maximal as follows.

!code PART max_output!

This outputs:

!output PART max_output!

The solutions generated by `Search` are accessible using `get_solution`. The solutions are sorted descendingly: the best solution is at index 0, the worst solution is at index `s.num_solutions()-1`.

A best solution at index 0 is optimal when `s.is_optimal()` returns true. To know when the solution was generated, `sol.time` contains the number of seconds since the construction of the `Search` object.

The `sol.box()` method returns the value intervals of the features for which the output of the ensemble is unchanged. That is, for each possible assignment within the intervals, the trees always evaluate to the same leaf node (`s.get_solution_nodes`), and thus to the same output value. If a feature is missing from the box, it means that its value does not make a difference.

### Constrained Minimization

In this example, we constrain the first feature value to be between 3 and 5.
Because this is a very simple constraint, we can simply prune the search space before we start the search.

Although the constraint is simple, it is very useful. The exact same pruning strategy is used for l-infinity robustenss checking.

!code PART min_output_constrained!

The output is:

!output PART min_output_constrained!

We minimize by maximizing the negated ensemble, i.e., the ensemble where all leaf values are negated.

The pruning simply removes all leaf nodes with boxes that do not overlap with `prune_box` from the search.

### Contrasting Two Instances

In this example, we want to know what the maximum difference between the outputs of two instances can be when only the third feature is different, and first and second feature values are the same.

We achieve this by renaming the feature IDs in one of the trees using a feature map or `FeatMap` object.

!code PART featmap!

Output:
!output PART featmap!

There are two differences between tree 1 and tree 3:

- the leaf values are negated (`concat_negated`)
- internal node 6 uses feature ID 2 in tree 1 and feature ID 5 in tree 3

The other feature IDs are the same. This has the effect of allowing the first two trees (corresponding to the first instance) to take on different values for feature 3 than the last two trees (corresponding to the second instance).

The renaming of the feature IDs is fascilitated by the `FeatMap` object.

!code PART print_featmap!
!output PART print_featmap!

The above gives all IDs used by the two instances. `FeatMap::share_all_features_between_instances` can be used share all feature values between the two intances. By default, each ID is unique.
Use `FeatMap::use_same_id_for` to share the same ID for two features, either between two instances, or for the same instance.
Use `FeatMap::transform` to apply the changes to an `AddTree`.

We can find the maximum difference between the outputs of the first and the second instance as follows:

!code PART two_instances!

Output:
!output PART two_instances!

The maximum output difference in this case is 10. The only possible variation is between leaf nodes 7 or 8 in the second tree.

Use `Search::step_for(duration_in_seconds, num_steps)` to let the search run for the given duration. Per `num_steps` steps, a snapshot is added to `Search::snapshots`. This can be used to track the following stats:

- time (`time`)
- number of steps executed so far (`num_steps`)
- number of solutions so far (`num_solutions`)
- number of search states expanded so far (`num_states`)
- best epsilon value (`eps`)
- the best bounds so far (`bounds`), a tuple containing lower bound, A\* upper bound, and ARA\* upper bound

### Checking Robustness

Before we check the robustness of a particular example, we'll first use Veritas to enumerate all possible output configurations of the additive tree ensemble. To do this, we simply run the search until `Search::steps` returns false, indicating that all search states have been visited.

!code PART robustness0!

!output PART robustness0!

The boxes above partition the input space. Remember that when a feature is not present in a box, it does not have an effect given the other feature values and can take on any value.

We will pick an example from box 6 with output -9:

!code PART robustness0_eval!

Output:
!output PART robustness0_eval!

We now try to find the distance to the closest adversarial example for which the output of the model is positive. We use `VeritasRobustnessSearch` for this. The arguments are:

- model to minimize or None
- model to maximize or None (use both for targeted attacks)
- the example
- the initial delta value used by the binary search

!code PART robustness1!

Output:
!output PART robustness1!

We can verify this result using the MILP approach (Kantchelian et al.'16):

!code PART robustness1_kan!

Output:
!output PART robustness1_kan LINES 1:2!

MILP indeed finds the same solution.

### One-hot constraint

We can tell Veritas that some of the features are the results of a one-hot encoded categorical feature using `Search::add_onehot_constraint`. This ensures that exactly one of the features is true at all times.

For this constructed example with only two one-hot encoded features, the total number of solutions is four, but two of them are invalid:

!code PART onehot0!

Output:

!output PART onehot0!

When we inform Veritas that exactly one of the two features must be true:

!code PART onehot1!

Output:

!output PART onehot1!

# Citate

[View API documentation](https://alexandersch12.github.io/veritas/) | [Veritas in action blog post](https://dtai.cs.kuleuven.be/sports/blog/versatile-verification-of-soccer-analytics-models/)

**Veritas** is a versatile verification tool for tree ensembles. You can use
Veritas to generate _adversarial examples_, check _robustness_, find _dominant
attributes_ or simply ask _domain specific questions_ about your model.

Veritas uses its own tree representation and does not assume a specific model format (like XGBoost's JSON dump).
This makes it easy to use with many tree/ensemble learners. A translation function is included for XGBoost ensembles.

For more information, refer to the paper:

> Versatile Verification of Tree Ensembles.
> Laurens Devos, Wannes Meert, and Jesse Davis.
> ICML 2021
> http://proceedings.mlr.press/v139/devos21a.html
